{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsVNHQ927IJR",
        "outputId": "c96a54b6-779b-4490-bc4f-80e804c04f4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing cnn.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile cnn.cu\n",
        "\n",
        "#include <cuda_runtime.h>\n",
        "#include <iostream>\n",
        "\n",
        "#define KERNEL_SIZE 3\n",
        "#define INPUT_CHANNELS 3\n",
        "#define OUTPUT_CHANNELS 2\n",
        "#define INPUT_WIDTH 5\n",
        "#define INPUT_HEIGHT 5\n",
        "#define OUTPUT_WIDTH (INPUT_WIDTH - KERNEL_SIZE + 1)\n",
        "#define OUTPUT_HEIGHT (INPUT_HEIGHT - KERNEL_SIZE + 1)\n",
        "\n",
        "#define CHECK_CUDA_ERROR(err) \\\n",
        "    if (err != cudaSuccess) { \\\n",
        "        std::cerr << \"CUDA error: \" << cudaGetErrorString(err) << \" at line \" << __LINE__ << std::endl; \\\n",
        "        exit(EXIT_FAILURE); \\\n",
        "    }\n",
        "\n",
        "__global__ void conv_input_grad_kernel(float* d_input, const float* d_output, const float* weights,\n",
        "                                       int in_channels, int out_channels,\n",
        "                                       int input_width, int input_height,\n",
        "                                       int output_width, int output_height,\n",
        "                                       int kernel_size) {\n",
        "    int ic = blockIdx.z;\n",
        "    int x = blockIdx.x * blockDim.x + threadIdx.x;\n",
        "    int y = blockIdx.y * blockDim.y + threadIdx.y;\n",
        "\n",
        "    if (x >= input_width || y >= input_height || ic >= in_channels) return;\n",
        "\n",
        "    float value = 0.0f;\n",
        "    for (int oc = 0; oc < out_channels; ++oc) {\n",
        "        for (int i = 0; i < kernel_size; ++i) {\n",
        "            for (int j = 0; j < kernel_size; ++j) {\n",
        "                int out_x = x - i;\n",
        "                int out_y = y - j;\n",
        "                if (out_x >= 0 && out_x < output_width && out_y >= 0 && out_y < output_height) {\n",
        "                    int out_idx = oc * output_width * output_height + out_y * output_width + out_x;\n",
        "                    int weight_idx = oc * in_channels * kernel_size * kernel_size + ic * kernel_size * kernel_size + i * kernel_size + j;\n",
        "                    value += d_output[out_idx] * weights[weight_idx];\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "    int input_idx = ic * input_width * input_height + y * input_width + x;\n",
        "    d_input[input_idx] = value;\n",
        "\n",
        "    printf(\"input_grad[%d][%d][%d] = %f\\n\", ic, y, x, value);\n",
        "}\n",
        "\n",
        "__global__ void conv_weight_grad_kernel(float* d_weights, const float* d_output, const float* input,\n",
        "                                        int in_channels, int out_channels,\n",
        "                                        int input_width, int input_height,\n",
        "                                        int output_width, int output_height,\n",
        "                                        int kernel_size) {\n",
        "    int oc = blockIdx.z;\n",
        "    int ic = blockIdx.y;\n",
        "    int kx = threadIdx.x;\n",
        "    int ky = threadIdx.y;\n",
        "\n",
        "    if (kx >= kernel_size || ky >= kernel_size || ic >= in_channels || oc >= out_channels) return;\n",
        "\n",
        "    float value = 0.0f;\n",
        "    for (int y = 0; y < output_height; ++y) {\n",
        "        for (int x = 0; x < output_width; ++x) {\n",
        "            int out_idx = oc * output_width * output_height + y * output_width + x;\n",
        "            int in_x = x + kx;\n",
        "            int in_y = y + ky;\n",
        "            int input_idx = ic * input_width * input_height + in_y * input_width + in_x;\n",
        "            value += d_output[out_idx] * input[input_idx];\n",
        "        }\n",
        "    }\n",
        "    int weight_idx = oc * in_channels * kernel_size * kernel_size + ic * kernel_size * kernel_size + ky * kernel_size + kx;\n",
        "    d_weights[weight_idx] = value;\n",
        "\n",
        "    printf(\"weight_grad[%d][%d][%d][%d] = %f\\n\", oc, ic, ky, kx, value);\n",
        "}\n",
        "\n",
        "void launch_convolution_backward_kernels(float* d_input, float* d_weights, const float* d_output, const float* input, const float* weights) {\n",
        "    dim3 blockDim_input(16, 16);\n",
        "    dim3 gridDim_input((INPUT_WIDTH + 15) / 16, (INPUT_HEIGHT + 15) / 16, INPUT_CHANNELS);\n",
        "    conv_input_grad_kernel<<<gridDim_input, blockDim_input>>>(d_input, d_output, weights,\n",
        "        INPUT_CHANNELS, OUTPUT_CHANNELS,\n",
        "        INPUT_WIDTH, INPUT_HEIGHT,\n",
        "        OUTPUT_WIDTH, OUTPUT_HEIGHT,\n",
        "        KERNEL_SIZE);\n",
        "    CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "\n",
        "    dim3 blockDim_weight(KERNEL_SIZE, KERNEL_SIZE);\n",
        "    dim3 gridDim_weight(1, INPUT_CHANNELS, OUTPUT_CHANNELS);\n",
        "    conv_weight_grad_kernel<<<gridDim_weight, blockDim_weight>>>(d_weights, d_output, input,\n",
        "        INPUT_CHANNELS, OUTPUT_CHANNELS,\n",
        "        INPUT_WIDTH, INPUT_HEIGHT,\n",
        "        OUTPUT_WIDTH, OUTPUT_HEIGHT,\n",
        "        KERNEL_SIZE);\n",
        "    CHECK_CUDA_ERROR(cudaGetLastError());\n",
        "}\n",
        "\n",
        "int main() {\n",
        "    float *d_input, *d_weights, *d_output, *input, *weights;\n",
        "    size_t input_size = INPUT_CHANNELS * INPUT_WIDTH * INPUT_HEIGHT * sizeof(float);\n",
        "    size_t output_size = OUTPUT_CHANNELS * OUTPUT_WIDTH * OUTPUT_HEIGHT * sizeof(float);\n",
        "    size_t weight_size = OUTPUT_CHANNELS * INPUT_CHANNELS * KERNEL_SIZE * KERNEL_SIZE * sizeof(float);\n",
        "\n",
        "    cudaMalloc(&d_input, input_size);\n",
        "    cudaMalloc(&d_output, output_size);\n",
        "    cudaMalloc(&d_weights, weight_size);\n",
        "\n",
        "    cudaMalloc(&input, input_size);\n",
        "    cudaMalloc(&weights, weight_size);\n",
        "\n",
        "    // Fill d_output, input, and weights with dummy data if needed here\n",
        "\n",
        "    launch_convolution_backward_kernels(d_input, d_weights, d_output, input, weights);\n",
        "\n",
        "    cudaFree(d_input);\n",
        "    cudaFree(d_output);\n",
        "    cudaFree(d_weights);\n",
        "    cudaFree(input);\n",
        "    cudaFree(weights);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile with the specified architecture\n",
        "!nvcc cnn.cu -o cnn -gencode arch=compute_75,code=sm_75\n",
        "\n",
        "# Run the executable\n",
        "!./cnn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xTA1xKB9aGS",
        "outputId": "a4dab41c-b65a-4722-f94b-53330a8ee1b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_grad[2][4][0] = 0.000000\n",
            "input_grad[2][4][1] = 0.000000\n",
            "input_grad[2][4][2] = 0.000000\n",
            "input_grad[2][4][3] = 0.000000\n",
            "input_grad[2][4][4] = 0.000000\n",
            "input_grad[1][4][0] = 0.000000\n",
            "input_grad[1][4][1] = 0.000000\n",
            "input_grad[1][4][2] = 0.000000\n",
            "input_grad[1][4][3] = 0.000000\n",
            "input_grad[1][4][4] = 0.000000\n",
            "input_grad[0][4][0] = 0.000000\n",
            "input_grad[0][4][1] = 0.000000\n",
            "input_grad[0][4][2] = 0.000000\n",
            "input_grad[0][4][3] = 0.000000\n",
            "input_grad[0][4][4] = 0.000000\n",
            "input_grad[2][0][0] = 0.000000\n",
            "input_grad[2][0][1] = 0.000000\n",
            "input_grad[2][0][2] = 0.000000\n",
            "input_grad[2][0][3] = 0.000000\n",
            "input_grad[2][0][4] = 0.000000\n",
            "input_grad[2][1][0] = 0.000000\n",
            "input_grad[2][1][1] = 0.000000\n",
            "input_grad[2][1][2] = 0.000000\n",
            "input_grad[2][1][3] = 0.000000\n",
            "input_grad[2][1][4] = 0.000000\n",
            "input_grad[2][2][0] = 0.000000\n",
            "input_grad[2][2][1] = 0.000000\n",
            "input_grad[2][2][2] = 0.000000\n",
            "input_grad[2][2][3] = 0.000000\n",
            "input_grad[2][2][4] = 0.000000\n",
            "input_grad[2][3][0] = 0.000000\n",
            "input_grad[2][3][1] = 0.000000\n",
            "input_grad[2][3][2] = 0.000000\n",
            "input_grad[2][3][3] = 0.000000\n",
            "input_grad[2][3][4] = 0.000000\n",
            "input_grad[1][0][0] = 0.000000\n",
            "input_grad[1][0][1] = 0.000000\n",
            "input_grad[1][0][2] = 0.000000\n",
            "input_grad[1][0][3] = 0.000000\n",
            "input_grad[1][0][4] = 0.000000\n",
            "input_grad[1][1][0] = 0.000000\n",
            "input_grad[1][1][1] = 0.000000\n",
            "input_grad[1][1][2] = 0.000000\n",
            "input_grad[1][1][3] = 0.000000\n",
            "input_grad[1][1][4] = 0.000000\n",
            "input_grad[1][2][0] = 0.000000\n",
            "input_grad[1][2][1] = 0.000000\n",
            "input_grad[1][2][2] = 0.000000\n",
            "input_grad[1][2][3] = 0.000000\n",
            "input_grad[1][2][4] = 0.000000\n",
            "input_grad[1][3][0] = 0.000000\n",
            "input_grad[1][3][1] = 0.000000\n",
            "input_grad[1][3][2] = 0.000000\n",
            "input_grad[1][3][3] = 0.000000\n",
            "input_grad[1][3][4] = 0.000000\n",
            "input_grad[0][0][0] = 0.000000\n",
            "input_grad[0][0][1] = 0.000000\n",
            "input_grad[0][0][2] = 0.000000\n",
            "input_grad[0][0][3] = 0.000000\n",
            "input_grad[0][0][4] = 0.000000\n",
            "input_grad[0][1][0] = 0.000000\n",
            "input_grad[0][1][1] = 0.000000\n",
            "input_grad[0][1][2] = 0.000000\n",
            "input_grad[0][1][3] = 0.000000\n",
            "input_grad[0][1][4] = 0.000000\n",
            "input_grad[0][2][0] = 0.000000\n",
            "input_grad[0][2][1] = 0.000000\n",
            "input_grad[0][2][2] = 0.000000\n",
            "input_grad[0][2][3] = 0.000000\n",
            "input_grad[0][2][4] = 0.000000\n",
            "input_grad[0][3][0] = 0.000000\n",
            "input_grad[0][3][1] = 0.000000\n",
            "input_grad[0][3][2] = 0.000000\n",
            "input_grad[0][3][3] = 0.000000\n",
            "input_grad[0][3][4] = 0.000000\n",
            "weight_grad[1][1][0][0] = 0.000000\n",
            "weight_grad[1][1][0][1] = 0.000000\n",
            "weight_grad[1][1][0][2] = 0.000000\n",
            "weight_grad[1][1][1][0] = 0.000000\n",
            "weight_grad[1][1][1][1] = 0.000000\n",
            "weight_grad[1][1][1][2] = 0.000000\n",
            "weight_grad[1][1][2][0] = 0.000000\n",
            "weight_grad[1][1][2][1] = 0.000000\n",
            "weight_grad[1][1][2][2] = 0.000000\n",
            "weight_grad[0][0][0][0] = 0.000000\n",
            "weight_grad[0][0][0][1] = 0.000000\n",
            "weight_grad[0][0][0][2] = 0.000000\n",
            "weight_grad[0][0][1][0] = 0.000000\n",
            "weight_grad[0][0][1][1] = 0.000000\n",
            "weight_grad[0][0][1][2] = 0.000000\n",
            "weight_grad[0][0][2][0] = 0.000000\n",
            "weight_grad[0][0][2][1] = 0.000000\n",
            "weight_grad[0][0][2][2] = 0.000000\n",
            "weight_grad[1][2][0][0] = 0.000000\n",
            "weight_grad[1][2][0][1] = 0.000000\n",
            "weight_grad[1][2][0][2] = 0.000000\n",
            "weight_grad[1][2][1][0] = 0.000000\n",
            "weight_grad[1][2][1][1] = 0.000000\n",
            "weight_grad[1][2][1][2] = 0.000000\n",
            "weight_grad[1][2][2][0] = 0.000000\n",
            "weight_grad[1][2][2][1] = 0.000000\n",
            "weight_grad[1][2][2][2] = 0.000000\n",
            "weight_grad[0][1][0][0] = 0.000000\n",
            "weight_grad[0][1][0][1] = 0.000000\n",
            "weight_grad[0][1][0][2] = 0.000000\n",
            "weight_grad[0][1][1][0] = 0.000000\n",
            "weight_grad[0][1][1][1] = 0.000000\n",
            "weight_grad[0][1][1][2] = 0.000000\n",
            "weight_grad[0][1][2][0] = 0.000000\n",
            "weight_grad[0][1][2][1] = 0.000000\n",
            "weight_grad[0][1][2][2] = 0.000000\n",
            "weight_grad[1][0][0][0] = 0.000000\n",
            "weight_grad[1][0][0][1] = 0.000000\n",
            "weight_grad[1][0][0][2] = 0.000000\n",
            "weight_grad[1][0][1][0] = 0.000000\n",
            "weight_grad[1][0][1][1] = 0.000000\n",
            "weight_grad[1][0][1][2] = 0.000000\n",
            "weight_grad[1][0][2][0] = 0.000000\n",
            "weight_grad[1][0][2][1] = 0.000000\n",
            "weight_grad[1][0][2][2] = 0.000000\n",
            "weight_grad[0][2][0][0] = 0.000000\n",
            "weight_grad[0][2][0][1] = 0.000000\n",
            "weight_grad[0][2][0][2] = 0.000000\n",
            "weight_grad[0][2][1][0] = 0.000000\n",
            "weight_grad[0][2][1][1] = 0.000000\n",
            "weight_grad[0][2][1][2] = 0.000000\n",
            "weight_grad[0][2][2][0] = 0.000000\n",
            "weight_grad[0][2][2][1] = 0.000000\n",
            "weight_grad[0][2][2][2] = 0.000000\n"
          ]
        }
      ]
    }
  ]
}